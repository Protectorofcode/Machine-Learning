{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Линейные модели классификации и регрессии\n",
    "## <center>Логистическая регрессия и метод максимального правдоподобия "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Линейный классификатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основная идея линейного классификатора заключается в том, что признаковое пространство может быть разделено гиперплоскостью на две полуплоскости, в каждой из которых прогнозируется одно из двух значений целевого класса. \n",
    "Если это можно сделать без ошибок, то обучающая выборка называется *линейно разделимой*.\n",
    "\n",
    "Мы уже знакомы с линейной регрессией и методом наименьших квадратов. Рассмотрим задачу бинарной классификации, причем метки целевого класса обозначим \"+1\" (положительные примеры) и \"-1\" (отрицательные примеры).\n",
    "Один из самых простых линейных классификаторов получается на основе регрессии вот таким образом:\n",
    "\n",
    "$$\\Large a(\\textbf{x}) = \\text{sign}(\\textbf{w}^{\\text{T}}\\textbf x),$$\n",
    "\n",
    "где\n",
    " - $\\textbf{x}$ – вектор признаков примера (вместе с единицей);\n",
    " - $\\textbf{w}$ – веса в линейной модели (вместе со смещением $w_0$);\n",
    " - $\\text{sign}(\\bullet)$ – функция \"сигнум\", возвращающая знак своего аргумента;\n",
    " - $a(\\textbf{x})$ – ответ классификатора на примере $\\textbf{x}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Логистическая регрессия как линейный классификатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логистическая регрессия является частным случаем линейного классификатора, но она обладает хорошим \"умением\" – прогнозировать вероятность $p_+$ отнесения примера $\\textbf{x}_\\text{i}$ к классу \"+\":\n",
    "$$\\Large p_+ = \\text P\\left(y_i = 1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) $$\n",
    "\n",
    "Прогнозирование не просто ответа (\"+1\" или \"-1\"), а именно *вероятности* отнесения к классу \"+1\" во многих задачах является очень важным бизнес-требованием. Например, в задаче кредитного скоринга, где традиционно применяется логистическая регрессия, часто прогнозируют вероятность невозврата кредита ($p_+$). Клиентов, обратившихся за кредитом, сортируют по этой предсказанной вероятности (по убыванию), и получается скоркарта — по сути, рейтинг клиентов от плохих к хорошим. Ниже приведен игрушечный пример такой скоркарты. \n",
    "\n",
    "Банк выбирает для себя порог $p_*$ предсказанной вероятности невозврата кредита (на картинке – $0.15$) и начиная с этого значения уже не выдает кредит. Более того, можно умножить предсказнную вероятность на выданную сумму и получить матожидание потерь с клиента, что тоже будет хорошей бизнес-метрикой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, мы хотим прогнозировать вероятность $p_+ \\in [0,1]$, а пока умеем строить линейный прогноз с помощью МНК: $b(\\textbf{x}) = \\textbf{w}^\\text{T} \\textbf{x} \\in \\mathbb{R}$. Каким образом преобразовать полученное значение в вероятность, пределы которой – [0, 1]? Очевидно, для этого нужна некоторая функция $f: \\mathbb{R} \\rightarrow [0,1].$ В модели логистической регрессии для этого берется конкретная функция: $\\sigma(z) = \\frac{1}{1 + \\exp^{-z}}$. И сейчас разберемся, каковы для этого предпосылки. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma(z):\n",
    "    return 1. / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xx = np.linspace(-10, 10, 1000)\n",
    "plt.plot(xx, [sigma(x) for x in xx]);\n",
    "plt.xlabel('z');\n",
    "plt.ylabel('sigmoid(z)')\n",
    "plt.title('Sigmoid function');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обозначим $P(X)$ вероятностью происходящего события $X$. Тогда отношение вероятностей $OR(X)$ определяется из $\\frac{P(X)}{1-P(X)}$, а это — отношение вероятностей того, произойдет ли событие или не произойдет. Очевидно, что вероятность и отношение шансов содержат одинаковую информацию. Но в то время как $P(X)$ находится в пределах от 0 до 1, $OR(X)$ находится в пределах от 0 до $\\infty$.\n",
    "\n",
    "Если вычислить логарифм $OR(X)$ (то есть называется логарифм шансов, или логарифм отношения вероятностей), то легко заметить, что $\\log{OR(X)} \\in \\mathbb{R}$. Его то мы и будем прогнозировать с помощью МНК.\n",
    "\n",
    "Посмотрим, как логистическая регрессия будет делать прогноз $p_+ = \\text{P}\\left(y_i = 1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right)$ (пока считаем, что веса $\\textbf{w}$ мы как-то получили (т.е. обучили модель), далее разберемся, как именно). \n",
    "\n",
    "**Шаг 1.** Вычислить значение $w_{0}+w_{1}x_1 + w_{2}x_2 + ... = \\textbf{w}^\\text{T}\\textbf{x}$. (уравнение $\\textbf{w}^\\text{T}\\textbf{x} = 0$ задает гиперплоскость, разделяющую примеры на 2 класса);\n",
    "\n",
    "\n",
    "**Шаг 2.** Вычислить логарифм отношения шансов: $ \\log(OR_{+}) =  \\textbf{w}^\\text{T}\\textbf{x}$.\n",
    "\n",
    "**Шаг 3.** Имея прогноз шансов на отнесение к классу \"+\" – $OR_{+}$, вычислить $p_{+}$ с помощью простой зависимости:\n",
    "\n",
    "$$\\Large p_{+} = \\frac{OR_{+}}{1 + OR_{+}} = \\frac{\\exp^{\\textbf{w}^\\text{T}\\textbf{x}}}{1 + \\exp^{\\textbf{w}^\\text{T}\\textbf{x}}} =  \\frac{1}{1 + \\exp^{-\\textbf{w}^\\text{T}\\textbf{x}}} = \\sigma(\\textbf{w}^\\text{T}\\textbf{x})$$\n",
    "\n",
    "\n",
    "В правой части мы получили как раз сигмоид-функцию.\n",
    "\n",
    "Итак, логистическая регрессия прогнозирует вероятность отнесения примера к классу \"+\" (при условии, что мы знаем его признаки и веса модели) как сигмоид-преобразование линейной комбинации вектора весов модели и вектора признаков примера:\n",
    "\n",
    "$$\\Large p_+(x_i) = \\text{P}\\left(y_i = 1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) = \\sigma(\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}). $$\n",
    "\n",
    "Следующий вопрос: как модель обучается. Тут мы опять обращаемся к принципу максимального правдоподобия."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Принцип максимального правдоподобия и логистическая регрессия\n",
    "Теперь посмотрим, как из принципа максимального правдоподобия получается оптимизационная задача, которую решает логистическая регрессия, а именно, – минимизация *логистической* функции потерь. \n",
    "Только что мы увидели, что логистическая регрессия моделирует вероятность отнесения примера к классу \"+\" как \n",
    "\n",
    "$$\\Large p_+(\\textbf{x}_\\text{i}) = \\text{P}\\left(y_i = 1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) = \\sigma(\\textbf{w}^\\text{T}\\textbf{x}_\\text{i})$$\n",
    "\n",
    "Тогда для класса \"-\" аналогичная вероятность:\n",
    "$$\\Large p_-(\\textbf{x}_\\text{i})  = \\text{P}\\left(y_i = -1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right)  = 1 - \\sigma(\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}) = \\sigma(-\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}) $$\n",
    "\n",
    "Оба этих выражения можно ловко объединить в одно (следите за моими руками – не обманывают ли вас):\n",
    "\n",
    "$$\\Large \\text{P}\\left(y = y_i \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) = \\sigma(y_i\\textbf{w}^\\text{T}\\textbf{x}_\\text{i})$$\n",
    "\n",
    "Выражение $M(\\textbf{x}_\\text{i}) = y_i\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}$ называется *отступом* (*margin*) классификации на объекте $\\textbf{x}_\\text{i}$ (не путать с зазором (тоже margin), про который чаще всего говорят в контексте SVM). Если он неотрицателен, модель не ошибается на объекте $\\textbf{x}_\\text{i}$, если же отрицателен – значит, класс для $\\textbf{x}_\\text{i}$  спрогнозирован неправильно. \n",
    "Заметим, что отступ определен для объектов именно обучающей выборки, для которых известны реальные метки целевого класса $y_i$.\n",
    "\n",
    "Чтобы понять, почему это мы сделали такие выводы, обратимся к геометрической интерпретации линейного классификатора. Подробно про это можно почитать в материалах Евгения Соколова – [тут](https://github.com/esokolov/ml-course-msu/blob/master/ML16/lecture-notes/Sem09_linear.pdf). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Рекомендую решить почти классическую задачу из начального курса линейной алгебры: найти расстояние от точки с радиус-вектором $\\textbf{x}_A$ до плоскости, которая задается уравнением $\\textbf{w}^\\text{T}\\textbf{x} = 0.$\n",
    "\n",
    "\n",
    "Ответ: \n",
    "$\\Large \\rho(\\textbf{x}_A, \\textbf{w}^\\text{T}\\textbf{x} = 0) = \\frac{\\textbf{w}^\\text{T}\\textbf{x}_A}{||\\textbf{w}||}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда получим (или посмотрим) ответ, то поймем, что чем больше по модулю выражение $\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}$, тем дальше точка $\\textbf{x}_\\text{i}$ находится от плоскости $\\textbf{w}^{\\text{T}}\\textbf{x} = 0.$\n",
    "\n",
    "Значит, выражение $M(\\textbf{x}_\\text{i}) = y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}$ – это своего рода \"уверенность\" модели в классификации объекта $\\textbf{x}_\\text{i}$: \n",
    "\n",
    "- если отступ большой (по модулю) и положительный, это значит, что метка класса поставлена правильно, а объект находится далеко от разделяющей гиперплоскости (такой объект классифицируется уверенно). На рисунке – $x_3$.\n",
    "- если отступ большой (по модулю) и отрицательный, значит метка класса поставлена неправильно, а объект находится далеко от разделюящей гиперплоскости (скорее всего такой объект – аномалия, например, его метка в обучающей выборке поставлена неправильно). На рисунке – $x_1$.\n",
    "- если отступ малый (по модулю), то объект находится близко к разделюящей гиперплоскости, а  знак отступа определяет, правильно ли объект классифицирован.  На рисунке – $x_2$ и $x_4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь распишем правдоподобие выборки, а именно, вероятность наблюдать данный вектор $\\textbf{y}$ у выборки $\\textbf X$. Делаем сильное предположение: объекты приходят независимо, из одного распределения (*i.i.d.*). Тогда\n",
    "\n",
    "$$\\Large \\text{P}\\left(\\textbf{y} \\mid \\textbf X, \\textbf{w}\\right) = \\prod_{i=1}^{\\ell} \\text{P}\\left(y = y_i \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right),$$\n",
    "\n",
    "где $\\ell$ – длина выборки $\\textbf X$ (число строк).\n",
    "\n",
    "Как водится, возьмем логарифм данного выражения (сумму оптимизировать намного проще, чем произведение):\n",
    "\n",
    "$$\\Large  \\log \\text{P}\\left(\\textbf{y} \\mid \\textbf X, \\textbf{w}\\right) = \\log \\sum_{i=1}^{\\ell} \\text{P}\\left(y = y_i \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) = \\log \\prod_{i=1}^{\\ell} \\sigma(y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i})   = $$\n",
    "\n",
    "$$\\Large  = \\sum_{i=1}^{\\ell} \\log \\sigma(y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}) = \\sum_{i=1}^{\\ell} \\log \\frac{1}{1 + \\exp^{-y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}}} = - \\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть в данном случае принцип максимизации правдоподобия приводит к минимизации выражения \n",
    "\n",
    "$$\\Large \\mathcal{L_{log}} (\\textbf X, \\textbf{y}, \\textbf{w}) = \\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}}).$$\n",
    "\n",
    "Это *логистическая* функция потерь, просуммированная по всем объектам обучающей выборки.\n",
    "\n",
    "Посмотрим на новую фунцию как на функцию от отступа: $L(M) = \\log (1 + \\exp^{-M})$. Нарисуем ее график, а также график 1/0 функциий потерь (*zero-one loss*), которая просто штрафует модель на 1 за ошибку на каждом объекте (отступ отрицательный): $L_{1/0}(M) = [M < 0]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Картинка отражает общую идею, что в задаче классификации, не умея напрямую минимизировать число ошибок (по крайней мере, градиентными методами это не сделать – производная 1/0 функциий потерь в нуле обращается в бесконечность), мы минимизируем некоторую ее верхнюю оценку. В данном случае это логистическая функция потерь (где логарифм двоичный, но это не принципиально), и справедливо \n",
    "\n",
    "$$\\Large \\mathcal{L_{\\text{1/0}}} (\\textbf X, \\textbf{y}, \\textbf{w}) = \\sum_{i=1}^{\\ell} [M(\\textbf{x}_\\text{i}) < 0] \\leq \\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}}) = \\mathcal{L_{\\log}} (\\textbf X, \\textbf{y}, \\textbf{w}), $$\n",
    "\n",
    "где $\\mathcal{L_{\\text{1/0}}} (\\textbf X, \\textbf{y}, \\textbf{w})$ – попросту число ошибок логистической регрессии с весами $\\textbf{w}$ на выборке $(\\textbf X, \\textbf{y})$.\n",
    "\n",
    "То есть уменьшая верхнюю оценку $\\mathcal{L_{\\log}}$ на число ошибок классификации, мы таким образом надеемся уменьшить и само число ошибок."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2-регуляризация логистической функции потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L2-регуляризация$ логистической регрессии устроена почти так же, как и в случае с гребневой (Ridge регрессией). Вместо функционала $\\mathcal{L_{\\log}} (X, \\textbf{y}, \\textbf{w})$ минимизируется следующий:\n",
    "\n",
    "$$\\Large J(\\textbf X, \\textbf{y}, \\textbf{w}) = \\mathcal{L_{\\log}} (\\textbf X, \\textbf{y}, \\textbf{w}) + \\lambda |\\textbf{w}|^2$$\n",
    "\n",
    "В случае логистической регрессии принято введение обратного коэффициента регуляризации $C = \\frac{1}{\\lambda}$. И тогда решением задачи будет\n",
    "\n",
    "$$\\Large \\widehat{\\textbf{w}}  = \\arg \\min_{\\textbf{w}} J(\\textbf X, \\textbf{y}, \\textbf{w}) =  \\arg \\min_{\\textbf{w}}\\ (C\\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}})+ |\\textbf{w}|^2)$$ \n",
    "\n",
    "Далее рассмотрим пример, позволяющий интуитивно понять один из смыслов регуляризации. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
